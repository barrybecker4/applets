<html>
<head>
<title>Barry Becker's Patents</title>

<meta name="generator" content="Created Using Yahoo! PageBuilder 2.61.90">
<meta name="author" content="Barry Becker">
<meta name="keywords" content="patents sgi Barry Becker">
</head>
<body background="http://www.geocities.com/clipart/pbi/backgrounds/Template_Business/travelagent_bg_3.gif" bgcolor="#FFFFFF" link="#0000FF" vlink="#660066" text="#000000"
>

<table border=0 cellspacing=0 cellpadding=0 width=642>
<tr valign="top" align="left">
	<td><img src="http://www.geocities.com/clipart/pbi/c.gif" height=1 width=8></td>
	<td><img src="http://www.geocities.com/clipart/pbi/c.gif" height=1 width=4></td>
	<td><img src="http://www.geocities.com/clipart/pbi/c.gif" height=1 width=535></td>
	<td><img src="http://www.geocities.com/clipart/pbi/c.gif" height=1 width=95></td>
	<td><img src="http://www.geocities.com/clipart/pbi/c.gif" height=1 width=1></td>
</tr>
<tr valign="top">
	<td colspan=5 height=3></td>
</tr>
<tr valign="top">
	<td colspan=2 height=46></td>
	<td colspan=1 rowspan=1 width=535><font face="Verdana" size="+3"><span style="font-size:36">Barry's Patents</span></font></td>
	<td colspan=2 height=46></td>
</tr>
<tr valign="top">
	<td colspan=5 height=38></td>
</tr>
<tr valign="top">
	<td colspan=1 height=1300></td>
	<td colspan=3 rowspan=1 width=634><span style="font-size:14"> &nbsp;&nbsp;&nbsp; Below are the patents that I worked on at SGI. All of these patents were created while developing </span><a href="http://www.the-data-mine.com/bin/view/Software/MineSet"><span style="font-size:14">MineSet</span></a><span style="font-size:14">, a data mining and visualization product. <br><br>&nbsp;&nbsp;&nbsp; A lot of business software could benefit from the introduction of machine learning algorithms. There are two main sorts: directed and undirected. Examples of directed learning are classifiers and regression. Examples of undirected learning are clustering (i.e. segmenation) and determining attribute importance. The primary difference is that directed learning implies that you know what you are searching for. <br>&nbsp;&nbsp;&nbsp; When there are hundreds of dimensions and measures it is very difficult for the user or the person configuring the system to know which to select for a specific chart. Classifiers can help a lot in this regard by showing at a high level all attributes and how they relate to a specific target. Furthermore, the visualization of classifiers helps with data integrity issues. It can tell you at a glance which of your dimensions or measures are not behaving the way you expect.<br><br>The patents that I worked on are:<br><br>&nbsp;&nbsp; * </span><a href="http://www.patentgenius.com/patent/5930803.html"><span style="font-size:14">Evidence Visualizer</span></a><span style="font-size:14"> - US patent number 5,930,803 (issued/granted July 1999; after a couple of years of processing). At the same time I submitted a patent application, I authored a paper with Ronny Kohavi and Dan Sommerfield titled&nbsp; </span><a href="http://www.box.net/shared/rsg6q948t7"><span style="font-size:14">Visualizing the Simple Bayesian Classifier</span></a><span style="font-size:14">. This paper shows the visualizer in an early form. The power of the naive-bayes classifier comes from smart binning of numerical attributes, and smart sorting and grouping of categorical attributes, with respect to some target. This tools was really great for getting an immediate overview of your data, but it also allowed interactive classification. <br><br>&nbsp;&nbsp; * </span><a href="http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBMQFjAA&url=http%3A%2F%2Fwww.wikipatents.com%2FUS-Patent-6301579%2Fmethod-system-and-computer-program-product-for-visualizing-a-data&ei=FzdcTcThJo-ztwfewMj7Cw&usg=AFQjCNGofoxqY0j6lEWeGUvblh9RbHGymQ"><span style="font-size:14">Decision Table Visualizer</span></a><span style="font-size:14">. US Patent number 6,301,579. Issued around 2000. This is similar to the Evidence Visualizer in that we are also visualizing a classifier. It kind of looks like a graphical representation of a giant pivot table with many dimensions or binned measures for the rows and columns. One advantage of the decision table classifier and it visualization is that it does not assume attribute independence like Naive Bayes. The reason is that it selects pairs of attributes in decreasing order of importance with respect to the target. In the visualization, it is possible to interactively drill into your data and do interactive classification just as was possible for the evidence classifier. The paper for this one is </span><a href="http://www.box.net/shared/zigmx08ik3"><span style="font-size:14">Visualizing Decision Table Classifiers</span></a><span style="font-size:14"> (</span><a href="http://www.box.net/shared/m8hzun8brh"><span style="font-size:14">pictures</span></a><span style="font-size:14">) are separate. This visualizer could be used to answer most of the same sorts of questions as the evidence classifier, but it can also show correlation between attributes.<br><br>&nbsp;&nbsp; * </span><a href="http://www.patentgenius.com/patent/5861891.html"><span style="font-size:14">Visual Approximation of Scattered data</span></a><span style="font-size:14">. US Patent number 5,861,891 (issued/granted Jan 1999; after a couple of years of processing) Allows creating a volumetric rendering of a 3D scaterplot when rendering the millions of individual points is prohibitive. The associated paper is </span><a href="http://www.box.net/shared/nq5psh0hl7"><span style="font-size:14">Volume Rendering for Relational Data</span></a><span style="font-size:14">. (</span><a href="http://www.box.net/shared/5p00k6bf2r"><span style="font-size:14">Pictures</span></a><span style="font-size:14">) are separate.<br><br>&nbsp;&nbsp; * </span><a href="http://www.wikipatents.com/US-Patent-6034697/interpolation-between-relational-tables-for-purposes-of-animating-a"><span style="font-size:14">Interpolation between relational tables for purposes of animating a data visualization</span></a><span style="font-size:14"> US Patent number 6,034,697. Issued about 1999. Closely related to the last one. It allowed animating the volume rendering of the relational data using sliders that represented other measures like time or age.<br><br>&nbsp;&nbsp; * </span><a href="http://ip.com/patent/US6373483"><span style="font-size:14">Method for approximating scattered data using color to represent values of a categorical variable</span></a><span style="font-size:14">  US Patent number 6,301,579. Issues around 2000. This is also closely related to the volume rendering for relational data idea. Here we adapt the technique to rendering scattplots that have a categorical (i.e. dimension instead of measure) variable mapped to color. The corresponding paper is </span><a href="http://www.box.net/shared/c3hyqjx9oy"><span style="font-size:14">Nominal Splats</span></a><span style="font-size:14">.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>Other SGI MineSet patents of possible interest.<br>&nbsp;&nbsp; * Attribute importance - don't have a reference yet.<br>&nbsp;&nbsp; * </span><a href="http://www.sgi.com/tech/mlc/trees.html"><span style="font-size:14">Tree visualizer</span></a><span style="font-size:14"> - Shows data in a tree hierarchy with a bar chart at each node. Used to show decision tree classifier in addition to straigh visualization.<br>&nbsp;&nbsp; * </span><a href="http://www.sgi.com/tech/mlc/index.html"><span style="font-size:14">MLC++</span></a><span style="font-size:14"> - MLC++ was a machine learning library written by Ronny Kohavi and Dan Sommerfield when they were at stanford. Its a bit dated now. We can probably recreate the algorithms.<br><br></span><a href="http://www.uspto.gov"><span style="font-size:14">US Patent Office</span></a></td>
	<td colspan=1 height=1300></td>
</tr>
</table>
</body>
</html>
